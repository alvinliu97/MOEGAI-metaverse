<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse</title>
  <link rel="icon" type="image/x-icon" href="static/images/favico.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Shiwen Mao, Dong In Kim</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Nanyang Technological University<br></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.03321" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->


                <!-- ArXiv abstract Link -->

                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the digital transformation era, Metaverse offers a fusion of virtual reality (VR), augmented reality (AR), and web technologies to create immersive digital experiences. However, the evolution of the Metaverse is slowed down by the challenges of content creation, scalability, and dynamic user interaction. Our study investigates an integration of Mixture of Experts (MoE) models with Generative Artificial Intelligence (GAI) for mobile edge computing to revolutionize content creation and interaction in the Metaverse. Specifically, we harness an MoE model's ability to efficiently manage complex data and complex tasks by dynamically selecting the most relevant experts running various sub-models to enhance the capabilities of GAI. We then present a novel framework that improves video content generation quality and consistency, and demonstrate its application through case studies. Our findings underscore the efficacy of MoE and GAI integration to redefine virtual experiences by offering a scalable, efficient pathway to harvest the Metaverse's full potential.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero">
  <div class="hero-body">
    <div class="container is-max-desktop content">
	
        <h1 id="video-generation">Video Generation</h1>
        <h3 id="install-diffuser-from-https-huggingface-co-docs-diffusers-en-installation">install diffuser from <a href="https://huggingface.co/docs/diffusers/en/installation">https://huggingface.co/docs/diffusers/en/installation</a></h3>
        <pre><code><span class="hljs-built_in">import</span> torch
        from diffusers <span class="hljs-built_in">import</span> DiffusionPipeline
        from diffusers.utils <span class="hljs-built_in">import</span> export_to_video
        
        <span class="hljs-attr">pipe</span> = DiffusionPipeline.from_pretrained(<span class="hljs-string">"damo-vilab/text-to-video-ms-1.7b"</span>, <span class="hljs-attr">torch_dtype=torch.float16,</span> <span class="hljs-attr">variant="fp16")</span>
        <span class="hljs-attr">pipe</span> = pipe.to(<span class="hljs-string">"cuda"</span>)
        
        <span class="hljs-attr">prompt</span> = <span class="hljs-string">"&lt;-----------&gt;"</span>
        <span class="hljs-attr">video_frames</span> = pipe(prompt).frames[<span class="hljs-number">0</span>]
        <span class="hljs-attr">video_path</span> = export_to_video(video_frames)
        video_path
        </code></pre><h1 id="video-evaluation">Video Evaluation</h1>
        <h3 id="install-with-pip">Install with pip</h3>
        <pre><code>pip <span class="hljs-keyword">install</span> vbench
        </code></pre><h3 id="install-with-git-clone">Install with git clone</h3>
        <pre><code>    git <span class="hljs-keyword">clone</span> <span class="hljs-title">https</span>://github.com/Vchitect/VBench.git
            pip install -r VBench/requirements.txt
            pip install VBench
        </code></pre><h2 id="evaluate-your-own-videos">Evaluate Your Own Videos</h2>
        <pre><code>python evaluate.py \
            -<span class="ruby">-dimension $DIMENSION \
        </span>    -<span class="ruby">-videos_path /path/to/folder_or_video/ \
        </span>    -<span class="ruby">-mode=custom_input</span>
        </code></pre><h1 id="moegai-metaverse-applications">MOEGAI-metaverse Applications</h1>
        <p><img src="MOE.png" alt="Applications of MoE in GAI for the Metaverse"></p>
        <p>This graph showcases various models and tasks, each cited with relevant research papers. Below, find detailed information about each model and task, accompanied by their respective Refs.</p>
        <h2 id="gan">GAN</h2>
        <h3 id="shared-bottom-moe">Shared Bottom MoE</h3>
        <ul>
        <li><p><strong>Task:</strong> Data Augmentation - Object Detection.
        <strong>Ref:</strong> <a href="https://ieeexplore.ieee.org/abstract/document/9745551/">Data Augmentation for Intelligent Mechanical Fault Diagnosis Based on Local Shared Multiple-Generator GAN</a></p>
        </li>
        <li><p><strong>Task:</strong> Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/beac6bfb7eac3d651307c16ac747df01-Paper-Conference.pdf">MCL-GAN: Generative Adversarial Networkswith Multiple Specialized Discriminators</a></p>
        </li>
        </ul>
        <h3 id="single-gate-moe">Single Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://arxiv.org/abs/1805.02481">Megan: Mixture of experts of generative adversarial networks for multimodal image generation</a></li>
        </ul>
        <h3 id="multiple-gate-moe">Multiple Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Speech Enhancement; Conetent Generation - Audio.
        <strong>Ref:</strong> <a href="https://ieeexplore.ieee.org/document/9980170">Perceptual Loss Function for Speech Enhancement Based on Generative Adversarial Learning</a></li>
        </ul>
        <h2 id="vae">VAE</h2>
        <h3 id="shared-bottom-moe">Shared Bottom MoE</h3>
        <ul>
        <li><strong>Task:</strong> Continual Learning.
        <strong>Ref:</strong> <a href="https://arxiv.org/abs/2110.12667">Mixture-of-Variational-Experts for Continual Learning</a></li>
        </ul>
        <h3 id="single-gate-moe">Single Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Behaviour Analysis - Anomaly Detection.
        <strong>Ref:</strong> <a href="https://link.springer.com/article/10.1007/s10489-020-01944-5">Mixture of experts with convolutional and variational autoencoders for anomaly detection</a></li>
        </ul>
        <h3 id="multiple-gate-moe">Multiple Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Cross Modality Content Generation.
        <strong>Ref:</strong> <a href="https://proceedings.neurips.cc/paper/2019/hash/0ae775a8cb3b499ad1fca944e6f5c836-Abstract.html">Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models</a></li>
        </ul>
        <h2 id="diffusion">Diffusion</h2>
        <h3 id="shared-bottom-moe">Shared Bottom MoE</h3>
        <ul>
        <li><strong>Task:</strong> Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.html">Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts</a></li>
        </ul>
        <h3 id="single-gate-moe">Single Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Super Resolution.
        <strong>Ref:</strong> <a href="https://arxiv.org/pdf/2310.12004.pdf">Image super-resolution via latent diffusion: A sampling-space mixture of experts and frequency-augmented decoder approach</a></li>
        </ul>
        <h3 id="multiple-gate-moe">Multiple Gate MoE</h3>
        <ul>
        <li><strong>Task:</strong> Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/821655c7dc4836838cd8524d07f9d6fd-Abstract-Conference.html">RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</a></li>
        </ul>
        <h2 id="transformer">Transformer</h2>
        <h3 id="shared-bottom-moe">Shared Bottom MoE</h3>
        <ul>
        <li><strong>Task:</strong> Knowledge Domain Shift; Prompt enhancement.
        <strong>Ref:</strong> <a href="https://arxiv.org/abs/2203.07285">Diversifying content generation for commonsense reasoning with mixture of knowledge graph experts</a></li>
        </ul>
        <h3 id="single-gate-moe">Single Gate MoE</h3>
        <ul>
        <li><p><strong>Task:</strong> Model Acceleration.
        <strong>Ref:</strong> <a href="https://proceedings.mlr.press/v162/rajbhandari22a.html">DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</a></p>
        </li>
        <li><p><strong>Task:</strong> Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://arxiv.org/abs/2204.08396">STABLEMOE: Stable Routing Strategy for Mixture of Experts</a></p>
        </li>
        </ul>
        <h3 id="multiple-gate-moe">Multiple Gate MoE</h3>
        <ul>
        <li><p><strong>Task:</strong> Model Acceleration; Conetent Generation - Image.
        <strong>Ref:</strong> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b653f34d576d1790481e3797cb740214-Abstract-Conference.html">MÂ³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</a></p>
        </li>
        <li><p><strong>Task:</strong> Image Classification.
        <strong>Ref:</strong> <a href="https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html">Scaling Vision with Sparse Mixture of Experts</a></p>
        </li>
        </ul>
       

    <h2>ðŸ“š Cite Our Work</h2>
    <p>If our work aids your research, please cite our work:</p>
    <pre><code>@misc{liu2024fusion,
        title={Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse}, 
        author={Guangyuan Liu and Hongyang Du and Dusit Niyato and Jiawen Kang and Zehui Xiong and Abbas Jamalipour and Shiwen Mao and Dong In Kim},
        year={2024},
        eprint={2404.03321},
        archivePrefix={arXiv}
}</code></pre>
	
</div>
</div>
</section>



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>